{
  "test_id": null,
  "request": {
    "method": "POST",
    "url": "http://0.0.0.0:11434/v1/v1/chat/completions",
    "headers": {},
    "body": {
      "model": "llama3.2:3b-instruct-fp16",
      "messages": [
        {
          "role": "user",
          "content": "Test trace openai with temperature 0"
        }
      ],
      "max_tokens": 100,
      "stream": false,
      "temperature": 0.7
    },
    "endpoint": "/v1/chat/completions",
    "model": "llama3.2:3b-instruct-fp16"
  },
  "response": {
    "body": {
      "__type__": "openai.types.chat.chat_completion.ChatCompletion",
      "__data__": {
        "id": "rec-b2c646582d0a",
        "choices": [
          {
            "finish_reason": "length",
            "index": 0,
            "logprobs": null,
            "message": {
              "content": "I can guide you on how to test a model like OpenAI's with a temperature parameter set to 0. The idea is to understand what happens when the temperature is very low, which should result in very similar or deterministic outputs.\n\nFor this example, I'll use a simple language model (like a large language model) and not directly access an actual OpenAI API due to their usage policies. We can simulate the behavior using Python and a library like Hugging Face's Transformers for general-purpose N",
              "refusal": null,
              "role": "assistant",
              "annotations": null,
              "audio": null,
              "function_call": null,
              "tool_calls": null
            }
          }
        ],
        "created": 0,
        "model": "llama3.2:3b-instruct-fp16",
        "object": "chat.completion",
        "service_tier": null,
        "system_fingerprint": "fp_ollama",
        "usage": {
          "completion_tokens": 100,
          "prompt_tokens": 33,
          "total_tokens": 133,
          "completion_tokens_details": null,
          "prompt_tokens_details": null
        }
      }
    },
    "is_streaming": false
  }
}
